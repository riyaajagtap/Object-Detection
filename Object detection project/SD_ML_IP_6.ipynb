{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reference to the webcam :\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera_height = 500\n",
    "raw_frames_type_1 = []\n",
    "raw_frames_type_2 = []\n",
    "raw_frames_type_3 = []\n",
    "raw_frames_type_4 = []\n",
    "\n",
    "while(True):\n",
    "    #read a new frame\n",
    "    _, frame = camera.read()\n",
    "    \n",
    "    #flip the frame \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    #rescaling camera output\n",
    "    aspect = frame.shape[1] / float(frame.shape[0])\n",
    "    res = int(aspect * camera_height) #landscape orientation - wide image\n",
    "    frame = cv2.resize(frame, (res, camera_height))\n",
    "    \n",
    "    # add rectangle\n",
    "    cv2.rectangle(frame, (300,75), (650, 425), (0, 255, 0), 2)\n",
    "    \n",
    "    # show the frame\n",
    "    cv2.imshow(\"Capturing Frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # quit camera if 'q' is pressed\n",
    "    if key & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    elif key & 0xFF == ord(\"1\"):\n",
    "        #save the frame\n",
    "        raw_frames_type_1.append(frame)\n",
    "        print('1 key pressed - saved TYPE_1 frame')\n",
    "        \n",
    "    elif key & 0xFF == ord(\"2\"):\n",
    "        #save the frame\n",
    "        raw_frames_type_2.append(frame)\n",
    "        print('2 key pressed - saved TYPE_2 frame')\n",
    "        \n",
    "    elif key & 0xFF == ord(\"3\"):\n",
    "        #save the frame\n",
    "        raw_frames_type_3.append(frame)\n",
    "        print('3 key pressed - saved TYPE_3 frame')\n",
    "        \n",
    "    elif key & 0xFF == ord(\"4\"):\n",
    "        #save the frame\n",
    "        raw_frames_type_4.append(frame)\n",
    "        print('4 key pressed - saved TYPE_4 frame')\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now crop Region of interest(ROI) from the frame\n",
    "\n",
    "for i, frame in enumerate(raw_frames_type_1):\n",
    "    #get ROI\n",
    "    roi = frame[75+2:425-2, 300+2:650-2]\n",
    "    \n",
    "    #parse BGR to RGB\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #resize to 399*399\n",
    "    roi = cv2.resize(roi, (399, 399))\n",
    "    \n",
    "    #store the image\n",
    "    cv2.imwrite('C:/Users/hp/Desktop/Software_Development/raw_frames_type_1/{}.png'.format(i), cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    \n",
    "for i, frame in enumerate(raw_frames_type_2):\n",
    "    #get ROI\n",
    "    roi = frame[75+2:425-2, 300+2:650-2]\n",
    "    \n",
    "    #parse BGR to RGB\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #resize to 399*399\n",
    "    roi = cv2.resize(roi, (399, 399))\n",
    "    \n",
    "    #store the image\n",
    "    cv2.imwrite('C:/Users/hp/Desktop/Software_Development/raw_frames_type_2/{}.png'.format(i), cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    \n",
    "for i, frame in enumerate(raw_frames_type_3):\n",
    "    #get ROI\n",
    "    roi = frame[75+2:425-2, 300+2:650-2]\n",
    "    \n",
    "    #parse BGR to RGB\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #resize to 399*399\n",
    "    roi = cv2.resize(roi, (399, 399))\n",
    "    \n",
    "    #store the image\n",
    "    cv2.imwrite('C:/Users/hp/Desktop/Software_Development/raw_frames_type_3/{}.png'.format(i), cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    \n",
    "for i, frame in enumerate(raw_frames_type_4):\n",
    "    #get ROI\n",
    "    roi = frame[75+2:425-2, 300+2:650-2]\n",
    "    \n",
    "    #parse BGR to RGB\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #resize to 399*399\n",
    "    roi = cv2.resize(roi, (399, 399))\n",
    "    \n",
    "    #store the image\n",
    "    cv2.imwrite('C:/Users/hp/Desktop/Software_Development/raw_frames_type_4/{}.png'.format(i), cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 96\n",
    "height = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from keras import preprocessing\n",
    "\n",
    "#load image type_1\n",
    "images_type_1 = []\n",
    "for image_path in glob('C:/Users/hp/Desktop/Software_Development/raw_frames_type_1/*.*'):\n",
    "    image = preprocessing.image.load_img(image_path,\n",
    "                                        target_size=(width, height))\n",
    "    x= preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    images_type_1.append(x)\n",
    "    \n",
    "    \n",
    "#load image type_2\n",
    "images_type_2 = []\n",
    "for image_path in glob('C:/Users/hp/Desktop/Software_Development/raw_frames_type_2/*.*'):\n",
    "    image = preprocessing.image.load_img(image_path,\n",
    "                                        target_size=(width, height))\n",
    "    x= preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    images_type_2.append(x)\n",
    "    \n",
    "    \n",
    "#load image type_3\n",
    "images_type_3 = []\n",
    "for image_path in glob('C:/Users/hp/Desktop/Software_Development/raw_frames_type_3/*.*'):\n",
    "    image = preprocessing.image.load_img(image_path,\n",
    "                                        target_size=(width, height))\n",
    "    x= preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    images_type_3.append(x)\n",
    "    \n",
    "    \n",
    "#load image type_4\n",
    "images_type_4 = []\n",
    "for image_path in glob('C:/Users/hp/Desktop/Software_Development/raw_frames_type_4/*.*'):\n",
    "    image = preprocessing.image.load_img(image_path,\n",
    "                                        target_size=(width, height))\n",
    "    x= preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    images_type_4.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_name = ['earbud', 'Phone', 'book', 'pen']\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i, x in enumerate(images_type_1[:5]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    image = preprocessing.image.array_to_img(x)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('{} image'.format(class_name[0]))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i, x in enumerate(images_type_2[:5]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    image = preprocessing.image.array_to_img(x)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('{} image'.format(class_name[1]))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i, x in enumerate(images_type_3[:5]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    image = preprocessing.image.array_to_img(x)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('{} image'.format(class_name[2]))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i, x in enumerate(images_type_4[:5]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    image = preprocessing.image.array_to_img(x)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('{} image'.format(class_name[3]))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "x_type_1 = np.array(images_type_1)\n",
    "x_type_2 = np.array(images_type_2)\n",
    "x_type_3 = np.array(images_type_3)\n",
    "x_type_4 = np.array(images_type_4)\n",
    "\n",
    "print(x_type_1.shape)\n",
    "print(x_type_2.shape)\n",
    "print(x_type_3.shape)\n",
    "print(x_type_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.concatenate((x_type_1, x_type_2), axis=0)\n",
    "\n",
    "if len(x_type_3):\n",
    "    x = np.concatenate((x, x_type_3), axis=0)\n",
    "    \n",
    "    \n",
    "if len(x_type_4):\n",
    "    x = np.concatenate((x, x_type_4), axis=0)\n",
    "    \n",
    "#scale the data to (0, 1) values\n",
    "x= x/ 255\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-180cdd5b5db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_type_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_type_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_type_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_type_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_type_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_type_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_type_1 = [0 for item in enumerate (x_type_1)]\n",
    "y_type_2 = [1 for item in enumerate (x_type_2)]\n",
    "y_type_3 = [2 for item in enumerate (x_type_3)]\n",
    "y_type_4 = [3 for item in enumerate (x_type_4)]\n",
    "\n",
    "y = np.concatenate((y_type_1, y_type_2), axis=0)\n",
    "\n",
    "if len(y_type_3):\n",
    "    y = np.concatenate((y, y_type_3), axis=0)\n",
    "    \n",
    "if len(y_type_4):\n",
    "    y = np.concatenate((y, y_type_4), axis=0)\n",
    "    \n",
    "    \n",
    "y = to_categorical(y, num_classes= len(class_name))\n",
    "\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "# what is CNN?\n",
    "#In deep learning, a convolutional neural network (CNN, or ConvNet) \n",
    "#is a class of deep neural networks, most commonly applied to analyzing visual imagery.\n",
    "\n",
    "# default parameters\n",
    "\n",
    "conv_1 = 16\n",
    "conv_1_drop = 0.2\n",
    "conv_2 = 32\n",
    "conv_2_drop = 0.2\n",
    "dense_1_n = 1024\n",
    "dense_1_drop = 0.2 #1.0 means no dropout, and 0.0 means no outputs from the layer.\n",
    "dense_2_n = 512\n",
    "dense_2_drop = 0.2 \n",
    "lr = 0.001 #learning rate\n",
    "\n",
    "#we want a low rate first to avoid jumping back and forth across the curve. Range(0.001-0.1)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "color_channels = 3\n",
    "\n",
    "def build_model(conv_1_drop = conv_1_drop, conv_2_drop = conv_2_drop,\n",
    "               dense_1_n = dense_1_n, dense_1_drop = dense_1_drop,\n",
    "                dense_2_n = dense_2_n, dense_2_drop = dense_2_drop,\n",
    "                lr = lr ):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(conv_1, (3, 3),\n",
    "                           input_shape = (width, height, color_channels),\n",
    "                           activation = 'relu'))\n",
    "#what is relu?\n",
    "# The rectified linear activation function or ReLU is a piecewise linear function \n",
    "#that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "#what is maxpoolin2d?\n",
    "#Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, \n",
    "#or largest, value in each patch of each feature map.\n",
    "\n",
    "#why is it used?\n",
    "# Pooling layers are used to reduce the dimensions of the feature maps. \n",
    "#Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. \n",
    "#The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(conv_1_drop))\n",
    "    \n",
    "#Droupout: Dropout is a technique where randomly selected neurons are ignored during training.\n",
    "#why is droupout used?\n",
    "#Deep learning neural networks are likely to quickly overfit a training dataset with few examples.\n",
    "#Dropout very computationally cheap and remarkably effective regularization method to reduce overfitting \n",
    "#and improve generalization\n",
    " \n",
    "#A 2D convolution layer means that the input of the convolution operation is three-dimensional, \n",
    "#for example, a color image which has a value for each pixel across three layers: red, blue and green.\n",
    "    \n",
    "    model.add(Convolution2D(conv_2, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(conv_2_drop))\n",
    " \n",
    "#Flattening is converting the data into a 1-dimensional array for inputting it to the next layer.\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "#Dense layer is the regular deeply connected neural network layer. It is most common and frequently used layer.\n",
    "    \n",
    "    model.add(Dense(dense_1_n, activation = 'relu'))\n",
    "    model.add(Dropout(dense_1_drop))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(dense_2_n, activation = 'relu'))\n",
    "    model.add(Dropout(dense_2_drop))\n",
    "    \n",
    "    model.add(Dense(len(class_name), activation='softmax')) \n",
    "    \n",
    "#Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities\n",
    "\n",
    "# It is a Softmax activation plus a Cross-Entropy loss. \n",
    "#If we use this loss, we will train a CNN to output a probability over the classes for each image.\n",
    "\n",
    "#Optimizer Adam: optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                 optimizer=Adam(lr=lr),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1) # for reproducibility\n",
    "\n",
    "# model with base parameters\n",
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('things_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x, y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "\n",
    "task_1 = preprocessing.image.load_img('C:/Users/hp/Desktop/Software_Development/raw_frames_type_1/0.png', \n",
    "                                   target_size=(96, 96))\n",
    "plt.imshow(task_1)\n",
    "plt.show()\n",
    "\n",
    "task_x = np.expand_dims(task_1, axis=0)\n",
    "predictions = model.predict(task_x)\n",
    "\n",
    "class_name[np.argmax(predictions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# get the reference to the webcm\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera_height = 500\n",
    "\n",
    "while(True):\n",
    "    #read a new frame\n",
    "    _, frame = camera.read()\n",
    "    \n",
    "    #flip the frame \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    #rescaling camera output\n",
    "    aspect = frame.shape[1] / float(frame.shape[0])\n",
    "    res = int(aspect * camera_height) #landscape orientation - wide image\n",
    "    frame = cv2.resize(frame, (res, camera_height))\n",
    "\n",
    "    # add rectangle\n",
    "    cv2.rectangle(frame, (300,75), (650, 425), (255, 255, 0), 2)\n",
    "    \n",
    "    #get ROI\n",
    "    roi = frame[75+2:425-2, 300+2:650-2]\n",
    "    \n",
    "    #parse BGR to RGB\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #resize to 96*96\n",
    "    roi = cv2.resize(roi, (96, 96))\n",
    "    #roi = inception_v3.preprocess_input(roi)\n",
    "    \n",
    "    #predict\n",
    "    task = np.expand_dims(roi, axis=0)\n",
    "    predictions = model.predict(task)\n",
    "    type_1_pred, type_2_pred, type_3_pred, type_4_pred = predictions[0]\n",
    "    \n",
    "    #add text\n",
    "    label_1 = '{} - {}%'.format(class_name[0], int(type_1_pred*100))\n",
    "    cv2.putText(frame, label_1, (70, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (20, 240, 150), 2)\n",
    "    \n",
    "    #add text\n",
    "    label_2 = '{} - {}%'.format(class_name[1], int(type_2_pred*100))\n",
    "    cv2.putText(frame, label_2, (70, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (20, 240, 240), 2)\n",
    "    \n",
    "    #add text\n",
    "    label_3 = '{} - {}%'.format(class_name[2], int(type_3_pred*100))\n",
    "    cv2.putText(frame, label_3, (70, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (240, 240, 240), 2)\n",
    "    \n",
    "    #add text\n",
    "    label_4 = '{} - {}%'.format(class_name[3], int(type_4_pred*100))\n",
    "    cv2.putText(frame, label_4, (70, 260), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 191, 0), 2)\n",
    "    \n",
    "    # show thw frame\n",
    "    cv2.imshow(\"Real time object detection\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # quit camera if 'q' key is pressed\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
